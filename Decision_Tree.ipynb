{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Implementation\n",
    "This notebook describes and displays all the code, results and observations of the Decision Tree Implementation from scratch.\n",
    "\n",
    "*Task 1*\n",
    "\n",
    "* First it describes the utils.py which has basic functions required for implementing the Decision Tree\n",
    "* Then it describes base.py which has the basic structure and other important function of the Decision Tree\n",
    "* Next we have metrics.py which has all the functions to measure and check the predicted results.\n",
    "* Following it to showcase the usage of Decision Tree on various test cases we have usage.py\n",
    "\n",
    "*Task 2*\n",
    "* For task 2 the classification-exp.py file is explained.\n",
    "\n",
    "*Task 3*\n",
    "* For task 3 the auto-efficiency.py is explained.\n",
    "\n",
    "*Task 4*\n",
    "* For task 4 the experiments.py file is explained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "*utils.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function for encoding all types of data(like Categorical data) to numerical data.\n",
    "def one_hot_encoding(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    return pd.get_dummies(X)\n",
    "\n",
    "# Function for checking whether a given series is real or discrete.\n",
    "# For this a threshold ratio of (unique values/total values) was defined apart from checking the data type of the series.\n",
    "def check_ifreal(y: pd.Series) -> bool:\n",
    "    if pd.api.types.is_numeric_dtype(y):\n",
    "        unique=len(y.unique())\n",
    "        total=y.count()\n",
    "        if (unique/total)<0.1:  \n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# For creating the function the formula for entropy was used\n",
    "def entropy(Y: pd.Series) -> float:\n",
    "    n=Y.size\n",
    "    classes=Y.unique()\n",
    "    H=0\n",
    "    for i in range(len(classes)):\n",
    "        # variable for storing the no. of occurences of a class\n",
    "        m=0\n",
    "        for j in range(len(Y)):\n",
    "            if Y.iloc[j]==classes[i]:\n",
    "                m+=1\n",
    "        p=m/n\n",
    "        H+=p*np.log2(p)\n",
    "    return -1*H\n",
    "\n",
    "# Mean squared Error- directly uses the formula\n",
    "def mse(Y: pd.Series) -> float:\n",
    "    avg=Y.mean()\n",
    "    sq_err=(Y-avg)**2\n",
    "    mean_sq_err=sq_err.mean()\n",
    "    return mean_sq_err\n",
    "\n",
    "# Gini index- uses the formula of gini index\n",
    "def gini(y: pd.Series) -> float:\n",
    "        classes = y.value_counts(normalize=True)\n",
    "        return 1 - np.sum(classes ** 2)\n",
    "\n",
    "# Information gain- Takes the output labels Y and input feature attr and the criterion to calculate info gain\n",
    "# Checks the criterion and accordingly calculates the info gain\n",
    "def information_gain(Y: pd.Series, attr: pd.Series, criterion: str) -> float:\n",
    "    Gain=0\n",
    "    if (criterion==\"entropy\"):\n",
    "        HS=entropy(Y)\n",
    "        Gain+=HS\n",
    "        c = attr.unique()\n",
    "        for i in range(len(c)):\n",
    "            # Si is a list for storing the Y values corresponding to a given class(values of attr.unique)\n",
    "            Si=[]\n",
    "            for j in range(len(attr)):\n",
    "                if attr.iloc[j] == c[i]:\n",
    "                    Si.append(Y.iloc[j])\n",
    "            Gain-=(len(Si)/len(Y))*entropy(pd.Series(Si))\n",
    "\n",
    "        return Gain\n",
    "    \n",
    "    elif criterion == \"gini_index\":\n",
    "        HS = gini(Y)\n",
    "        Gain = HS\n",
    "        c = attr.unique()\n",
    "        for i in range(len(c)):\n",
    "            # Si is a list for storing the Y values corresponding to a given class(values of attr.unique)\n",
    "            Si=[]\n",
    "            for j in range(len(attr)):\n",
    "                if attr.iloc[j] == c[i]:\n",
    "                    Si.append(Y.iloc[j])\n",
    "            Gain-=(len(Si)/len(Y))*gini(pd.Series(Si))\n",
    "\n",
    "        return Gain\n",
    "\n",
    "    elif criterion == \"mse\":\n",
    "        HS = mse(Y)\n",
    "        Gain = HS\n",
    "        c = attr.unique()\n",
    "        for i in range(len(c)):\n",
    "            Si = Y[attr == c[i]]\n",
    "            Gain -= (len(Si) / len(Y)) * mse(Si)\n",
    "            \n",
    "        return Gain\n",
    "\n",
    "\n",
    "\n",
    "def get_best_split(X: pd.Series, y: pd.Series,  real, criterion: str='information_gain') -> float:\n",
    "    best_split_value = None\n",
    "    best_score = -float('inf') \n",
    "\n",
    "    if (real):\n",
    "        sorted_indices = np.argsort(X)\n",
    "        sorted_X = X.iloc[sorted_indices]\n",
    "        sorted_y = y.iloc[sorted_indices]\n",
    "        \n",
    "        for i in range(1, len(sorted_X)):\n",
    "            split_value = (sorted_X.iloc[i-1] + sorted_X.iloc[i]) / 2\n",
    "            \n",
    "            left = sorted_X <= split_value\n",
    "            if (criterion=='information_gain'):\n",
    "                info_gain = information_gain(y, left, 'mse')\n",
    "            else:\n",
    "                info_gain = information_gain(y, left, 'mse')\n",
    "\n",
    "            if info_gain > best_score:\n",
    "                best_score = info_gain\n",
    "                best_split_value = split_value\n",
    "    else:\n",
    "        if (criterion=='information_gain'):\n",
    "            best_score = information_gain(y, X, 'mse')\n",
    "        else:\n",
    "            best_score = information_gain(y, X, 'mse')\n",
    "        \n",
    "    return best_split_value, best_score\n",
    "\n",
    "\n",
    "def get_best_val(X: pd.Series, y: pd.Series, real, criterion: str='information_gain') -> str:\n",
    "    best_val = None\n",
    "    best_info_gain = -float('inf')\n",
    "    \n",
    "    if (real==0):\n",
    "        if (criterion=='information_gain'):\n",
    "            best_info_gain=information_gain(y, X, 'entropy')\n",
    "        else:\n",
    "            best_info_gain=information_gain(y, X, 'gini_index')\n",
    "        \n",
    "    else:\n",
    "        sorted_indices = np.argsort(X)\n",
    "        sorted_X = X.iloc[sorted_indices]\n",
    "        sorted_y = y.iloc[sorted_indices]\n",
    "        \n",
    "        for i in range(1, len(sorted_X)):\n",
    "            split_value = (sorted_X.iloc[i-1] + sorted_X.iloc[i]) / 2\n",
    "            \n",
    "            left = sorted_X <= split_value\n",
    "            if (criterion=='information_gain'):\n",
    "                info_gain = information_gain(y, left, 'entropy')\n",
    "            else:\n",
    "                info_gain = information_gain(y, left, 'gini_index')\n",
    "\n",
    "            if info_gain > best_info_gain:\n",
    "                best_info_gain = info_gain\n",
    "                best_val = split_value\n",
    "            \n",
    "    return best_val, best_info_gain\n",
    "\n",
    "\n",
    "def opt_split_attribute(X: pd.DataFrame, y: pd.Series, features: pd.Series, real_target: bool, real_feature, criterion: str):\n",
    "    \"\"\"\n",
    "    Function to find the optimal attribute to split about.\n",
    "    If needed you can split this function into 2, one for discrete and one for real valued features.\n",
    "    You can also change the parameters of this function according to your implementation.\n",
    "\n",
    "    features: pd.Series is a list of all the attributes we have to split upon\n",
    "\n",
    "    return: attribute to split upon\n",
    "    \"\"\"\n",
    "\n",
    "    # According to whether the features are real or discrete valued and the criterion, find the attribute from the features series with the maximum information gain (entropy or variance based on the type of output) or minimum gini index (discrete output).\n",
    "    best_feature=None\n",
    "    best_split=None\n",
    "    best_info_gain=-float('inf')\n",
    "    if real_target:\n",
    "        c = 0\n",
    "        for i in features:\n",
    "            split, info_gain = get_best_split(X.loc[:,i], y, real_feature[c], criterion)\n",
    "            c += 1\n",
    "            if info_gain>best_info_gain:\n",
    "                best_info_gain=info_gain\n",
    "                best_split=split\n",
    "                best_feature=i\n",
    "    else:\n",
    "        c = 0\n",
    "        for i in features:\n",
    "            split, info_gain = get_best_val(X.loc[:,i], y, real_feature[c], criterion)\n",
    "            c += 1\n",
    "            if info_gain>best_info_gain:\n",
    "                best_info_gain=info_gain\n",
    "                best_split=split\n",
    "                best_feature=i\n",
    "    return best_feature, best_split\n",
    "\n",
    "\n",
    "def split_data(X: pd.DataFrame, y: pd.Series, attribute, value):\n",
    "    \"\"\"\n",
    "    Function to split the data according to an attribute.\n",
    "    Handles both discrete and real-valued features.\n",
    "    \"\"\"\n",
    "\n",
    "    if value is None:\n",
    "        # For discrete features\n",
    "        splits = []\n",
    "        unique_values = X[attribute].unique()\n",
    "        \n",
    "        for u in unique_values:\n",
    "            X_split = X[X[attribute] == u]\n",
    "            y_split = y[X[attribute] == u]\n",
    "            splits.append((X_split, y_split))\n",
    "    else:\n",
    "        # For real-valued features\n",
    "        X_split_left = X[X[attribute] <= value]\n",
    "        y_split_left = y[X[attribute] <= value]\n",
    "        \n",
    "        X_split_right = X[X[attribute] > value]\n",
    "        y_split_right = y[X[attribute] > value]\n",
    "        \n",
    "        splits = [(X_split_left, y_split_left), (X_split_right, y_split_right)]\n",
    "        \n",
    "    return splits\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
